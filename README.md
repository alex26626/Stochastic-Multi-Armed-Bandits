# Stochastic-Multi-Armed-Bandits
A stochastic multi-armed bandit problem consists in a sequential game between a player and an environment. More specifically, there exist K arms, each one of them characterized by its distribution of rewards. At each round, the player can choose which arm to pull, from which he will obtain its corresponding reward. Intuitively, the player’s objective is to maximize its cumulative reward over time, and, to do so, he needs to identify the best arm as soon as possible, to then keep on pulling it until the end of the game. 
Consequently, the player needs to face what’s commonly known as exploration vs exploitation dilemma in the field of Reinforcement Learning. According to the Weak Law of Large Numbers, the sample mean converges in probability to the true expected value, therefore, to have as accurate as possible estimates of the mean rewards, the player would need to pull each arm many times before being confident enough to select what he would think of as the best arm; as a consequence, he would spend many rounds playing suboptimal arms. On the other hand, after playing each arm a few times, he could decide to choose earlier the best arm and stick to it till the end of the game. However, in such case the estimated mean rewards would end up being more inaccurate, thus the probability for the player to choose a suboptimal arm would be higher. Bandit algorithms are built to tackle such dilemma, trying to balance exploration and exploitation.
The performance of each one of the algorithms is going to be empirically evaluated through different simulations, by considering both non-parametric and parametric bandit problems. In all scenarios, both the number of arms and the number of variables for each arm is set to ten.
For the non-parametric case, ten-dimensional discrete distributions are generated by a Dirichlet. For the simulations in this thesis, three cases have been considered: α=0.5 for moderately sparse distributions, α=0.1 for almost degenerate distributions, and α=10 for almost uniform distributions.
For the parametric case, three families of parametric probability distributions have been considered: Bernoulli, Poisson, and Exponential. Bernoulli rewards are a typical scenario in bandit problems, in which case pulling an arm would either give 0 or 1 as a reward. Therefore, identifying the arm with the highest mean translates into selecting the arm with the highest probability of giving 1 as a reward.
Both families are widely used to model natural and social phenomena, and they provide meaningful scenarios for both symmetrical and skewed distributions respectively. For each one, two cases are analysed: either each one of the arms is governed by a parameter in the range [1;10] in increasing order, or in the range [10;20]. The latter simulation, in addition to being characterized by distributions with higher means, is also bound to be a more difficult bandit problem given the higher variances, which cause more overlaps between the distributions. 
To empirically evaluate the performance of the bandit algorithms, each one is run for T rounds for M iterations, then the used regrets are obtained as the average of the cumulative regrets across all iterations. More specifically, with Dirichlet and Bernoulli bandits, for each iteration a new environment is generated; for instance, in the Dirichlet case with almost uniform distributions, for each iteration the environment has new rewards distributions, which have been sampled from a Dirichlet distribution with its parameter set to 10. Clearly, this is not possible with Poisson and Exponential distributions, which are deterministically generated by the chosen parameters.


